from typing import Callable, Any
import argparse
import textwrap

def add_common_transfer_parser(
    transfer_parser: argparse.ArgumentParser,
) -> tuple:
    """
    Add common arguments for all transfer learning parsers.
    """
    transfer_parser.set_defaults(func='transfer')
    transfer_optional = transfer_parser._action_groups.pop()
    transfer_optional.title = "Other arguments"
    transfer_required = transfer_parser.add_argument_group('Required arguments')
    transfer_data_args = transfer_parser.add_argument_group('Data-related arguments')
    transfer_model_args = transfer_parser.add_argument_group('Transfer learning arguments')
    transfer_learn_args = transfer_parser.add_argument_group('Learning-related arguments')
    transfer_raytune_args = transfer_parser.add_argument_group('RayTune-related arguments')
    
    transfer_required.add_argument('--ref_genome', type=str, metavar='FILE', default='',  
                          required=True, help=textwrap.dedent("""
                          File path of the reference genome in FASTA format.""").strip())
    
    transfer_required.add_argument('--train_data', type=str, metavar='FILE', default='',  
                          required=True, help= textwrap.dedent("""
                          File path of training data in a sorted BED format. If the options
                          --validation_data and --valid_ratio not specified, 10%% of the
                          sites sampled from the training BED will be used as the
                          validation data.""").strip())
    
    transfer_required.add_argument('--model_path', type=str, metavar='FILE', required=True,
                          help=textwrap.dedent("""
                          File path of the trained model.
                          """ ).strip())  
    
    transfer_required.add_argument('--model_config_path', type=str, metavar='FILE', required=True,
                          help=textwrap.dedent("""
                          File path for the configurations of the trained model.
                          """ ).strip())    
    
    transfer_model_args.add_argument('--train_all', default=False, action='store_true', 
                          help= textwrap.dedent("""
                          Train all parameters of the model. If False, only the parameters
                          in the last FC layers will be trained. Default: False.""").strip())
    
    
    transfer_model_args.add_argument('--init_fc_with_pretrained', default=False, action='store_true', 
                          help= textwrap.dedent("""
                          Use the weights of the pre-trained model to initialize the last 
                          FC layers. If False, parameters of last FC layers are randomly 
                          initialized. Default: False.""").strip())  

    transfer_data_args.add_argument('--validation_data', type=str, metavar='FILE', default=None,
                          help=textwrap.dedent("""
                          File path for validation data. If this option is set,
                          the value of --valid_ratio will be ignored. Default: None.
                          """).strip()) 
    
    transfer_data_args.add_argument('--sample_weights', type=str, metavar='FILE', default=None,
                          help=textwrap.dedent("""
                          File path for sample weights. Default: None.
                          """).strip())
    
    transfer_data_args.add_argument('--valid_ratio', type=float, metavar='FLOAT', default=0.1, 
                          help=textwrap.dedent("""
                          Ratio of validation data relative to the whole training data.
                          Default: 0.1. 
                          """ ).strip())
    
    transfer_data_args.add_argument('--split_seed', type=int, metavar='INT', default=-1, 
                          help=textwrap.dedent("""
                          Seed for randomly splitting data into training and validation
                          sets. Default: a random number generated by the job.
                          """ ).strip())
    
    transfer_data_args.add_argument('--save_valid_preds', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          Save prediction results for validation data in the checkpoint
                          folders. Default: False.
                          """ ).strip())
    
    transfer_data_args.add_argument('--bw_paths', type=str, metavar='FILE', default=None,
                          help=textwrap.dedent("""
                          File path for a list of BigWig files for non-sequence 
                          features such as the coverage track. If the pre-trained model
                          used some bigWig tracks, tracks with same names are needed 
                          to be provided with this option. Default: None.""").strip())
    
    transfer_data_args.add_argument('--with_h5', default=False, action='store_true', 
                          help=argparse.SUPPRESS)
    
    transfer_data_args.add_argument('--h5f_path', type=str, default=None,
                         help=argparse.SUPPRESS)
    
    transfer_data_args.add_argument('--n_h5_files', type=int, metavar='INT', default=1, 
                          help=argparse.SUPPRESS)

    transfer_learn_args.add_argument('--segment_center', type=int, metavar='INT', default=None, 
                          help=textwrap.dedent("""
                          The maximum encoding unit of the sequence. It affects trade-off 
                          between RAM memory and preprocessing speed. It is recommended to use 300k.
                          Default: None(read from config).""" ).strip())

    transfer_learn_args.add_argument('--sampled_segments', type=int, metavar='INT', default=None,
                          help=textwrap.dedent("""
                          Number of segments chosen for generating samples for batches in DataLoader.
                          Default: None(read from config).
                          """ ).strip())
    
    transfer_learn_args.add_argument('--batch_size', type=int, metavar='INT', default=[128], nargs='+', 
                          help=textwrap.dedent("""
                          Size of mini batches for model training. Default: 128.
                          """ ).strip())    
                          
    transfer_learn_args.add_argument('--custom_dataloader', default=False, action='store_true',  
                          help=textwrap.dedent("""
                          Specify the way to load data. Default: False.
                          """ ).strip())

    transfer_learn_args.add_argument('--optim', type=str, metavar='STR', default=['Adam'], nargs='+', 
                          help=textwrap.dedent("""
                          Optimization method for parameter learning: 'Adam' or 'AdamW'.
                          Default: 'Adam'.
                          """ ).strip())
 
    transfer_learn_args.add_argument('--learning_rate', type=float, metavar='FLOAT', default=[0.0001], nargs='+', 
                          help=textwrap.dedent("""
                          Learning rate for parameter learning, an argument for the 
                          optimization method.  Default: 0.0001.
                          """ ).strip())
    
    transfer_learn_args.add_argument('--lr_scheduler', type=str, metavar='STR', default=['StepLR'], nargs='+', 
                          help=textwrap.dedent("""
                          Learning rate scheduler.
                          Default: 'StepLR'.
                          """ ).strip())
    
    transfer_learn_args.add_argument('--weight_decay_auto', type=float, metavar='FLOAT', default=0.1, 
                          help=textwrap.dedent("""
                          Calcaute 'weight_decay' (regularization parameter) based on total 
                          training steps. It automatically adjusts 'weight_decay' for different 
                          batch sizes, training sizes and epochs. Its value MUST be smaller than 1.
                          For values in the range 0~1, smaller values mean stronger regularization.
                          Set a value of <=0 to turn this off.
                          Default: 0.1.
                          """ ).strip())
    
    transfer_learn_args.add_argument('--weight_decay', type=float, metavar='FLOAT', default=[1e-5], nargs='+', 
                          help=textwrap.dedent("""
                          'weight_decay' argument (regularization) for the optimization method. 
                          If you want to use this option, please also set '--weight_decay_auto' to 0.
                          Default: 1e-5.
                          """ ).strip())
    
    transfer_learn_args.add_argument('--restart_lr', type=float, metavar='FLOAT', default=1e-4, 
                          help=textwrap.dedent("""
                          When the learning rate reaches the mininum rate, reset it to 
                          a larger one. Default: 1e-4.
                          """ ).strip())
    
    transfer_learn_args.add_argument('--min_lr', type=float, metavar='FLOAT', default=1e-6, 
                          help=textwrap.dedent("""
                          The minimum learning rate. Default: 1e-6.
                          """ ).strip())
    
    transfer_learn_args.add_argument('--LR_gamma', type=float, metavar='FLOAT', default=[0.9], nargs='+', 
                          help=textwrap.dedent("""
                          'gamma' argument for the learning rate scheduler.
                           Default: 0.9.
                           """ ).strip())
    
    transfer_learn_args.add_argument('--cudnn_benchmark_false', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          If set, use only genomic sequences for the model and ignore
                          bigWig tracks. Default: False.""").strip())

    transfer_raytune_args.add_argument('--use_ray', default=False, action='store_true',
                          help=textwrap.dedent("""
                          Use ray to run multiple trials in parallel.  Default: False.
                          """ ).strip())

    transfer_raytune_args.add_argument('--experiment_name', type=str, metavar='STR', default='my_experiment',
                          help=textwrap.dedent("""
                          Ray-Tune experiment name.  Default: 'my_experiment'.
                          """ ).strip()) 
    
    transfer_raytune_args.add_argument('--n_trials', type=int, metavar='INT', default=2, 
                          help=textwrap.dedent("""
                          Number of trials for this training job.  Default: 2.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--epochs', type=int, metavar='INT', default=10, 
                          help=textwrap.dedent("""
                          Maximum number of epochs for each trial.  Default: 10.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--grace_period', type=int, metavar='INT', default=5, 
                          help=textwrap.dedent("""
                          'grace_period' parameter for early stopping. 
                           Default: 5.
                           """ ).strip())
    
    transfer_raytune_args.add_argument('--ASHA_metric', type=str, metavar='STR', default='loss', 
                          help=textwrap.dedent("""
                          Metric for ASHA schedualing; the value can be 'loss' or 'fdiri_loss'.
                          Default: 'loss'.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--ray_ncpus', type=int, metavar='INT', default=2, 
                          help=textwrap.dedent("""
                          Number of CPUs requested by Ray-Tune. Default: 2.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--ray_ngpus', type=int, metavar='INT', default=1, 
                          help=textwrap.dedent("""
                          Number of GPUs requested by Ray-Tune. Default: 1.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--cpu_per_trial', type=int, metavar='INT', default=2, 
                          help=textwrap.dedent("""
                          Number of CPUs used per trial. Default: 2.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--gpu_per_trial', type=float, metavar='FLOAT', default=0.15, 
                          help=textwrap.dedent("""
                          Number of GPUs used per trial. Default: 0.15.
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--cuda_id', type=str, metavar='STR', default='0', 
                          help=textwrap.dedent("""
                          Which GPU device to be used. Default: '0'. 
                          """ ).strip())
    
    transfer_raytune_args.add_argument('--rerun_failed', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          Rerun errored or incomplete trials. Default: False.
                          """ ).strip())

    transfer_optional.add_argument('--poisson_calib', default=False, action='store_true', 
                                  help=textwrap.dedent("""
                                  Use Poisson calibration for the model. 
                                  Default: False.""").strip())
 
    transfer_parser._action_groups.append(transfer_optional)
    
    return transfer_required, transfer_data_args, transfer_model_args, transfer_learn_args, transfer_raytune_args


def add_indel_transfer_parser(subparsers: argparse._SubParsersAction) -> argparse._SubParsersAction:
    """
    Add a parser for transfer learning of indel models.
    """

    transfer_parser = subparsers.add_parser(
        'transfer', 
        help='Transfer Learning for mural-indel model', 
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent("""
    Overview
    --------    
    This command uses learned weights from a pre-trained MuRaL-indel model to train new models. 
    The inputs include training and validation mutation data and training results will
    be saved under the "./experiment_name/" or "./ray_results/" folder.
    
    * Input data 
    The input files include the reference FASTA file (required), a training data file 
    (required), a validation data file (optional), and model-related files of a trained 
    model (required). The required model-related files are 'model' and 'model.config.pkl'
    under a specific checkpoint folder, which are normally produced by `mural_indel train` 
    or `mural_indel transfer`. 
   
    * Output data 
    Output data has the same structure as that of `mural_indel train`.

    Command line examples
    ---------------------
    1. The following command will train a transfer learning model using training data 
    in 'training.sorted.bed', the validation data in 'validation.sorted.bed', and the model
    files under 'checkpoint_9/'. All files are located in: example/indel
   
        mural_indel transfer --ref_genome data/seq.fa --train_data data/training.sorted.bed \\
        --validation_data data/validation.sorted.bed --model_path models/checkpoint_9/model \\
        --model_config_path models/checkpoint_9/model.config.pkl --train_all \\
        --init_fc_with_pretrained --experiment_name example4 > test4.out 2> test4.err
        """)
    )


    # Register common arguments
    transfer_required, transfer_data_args, transfer_model_args, transfer_learn_args, transfer_raytune_args = add_common_transfer_parser(transfer_parser)

    # Add indel-specific arguments
    transfer_model_args.add_argument('--n_class', type=int, metavar='INT', default=8,  
                                     help=textwrap.dedent("""
                                     Number of mutation classes (or types), including the 
                                     non-mutated class. Default: 8.""").strip())
    return transfer_parser


def add_snv_transfer_parser(subparsers: argparse._SubParsersAction) -> argparse._SubParsersAction:
    """
    Add a parser for transfer learning of snv models.
    """
    transfer_parser = subparsers.add_parser(
        'transfer', 
        help='Transfer Learning for mural-snv model', 
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=textwrap.dedent("""
    Overview
    --------    
    This command uses learned weights from a pre-trained MuRaL-snv model to train new models. 
    The inputs include training and validation mutation data and training results will
    be saved under the "./experiment_name/" or "./ray_results/" folder.
    
    * Input data 
    The input files include the reference FASTA file (required), a training data file 
    (required), a validation data file (optional), and model-related files of a trained 
    model (required). The required model-related files are 'model' and 'model.config.pkl'
    under a specific checkpoint folder, which are normally produced by `mural_snv train` 
    or `mural_snv transfer`. 
   
    * Output data 
    Output data has the same structure as that of `mural_snv train`.

    Command line examples
    ---------------------
    1. The following command will train a transfer learning model using training data 
    in 'training.sorted.bed', the validation data in 'validation.sorted.bed', and the model
    files under 'checkpoint_6/'. All files are located in: example/snv
   
        mural_snv transfer --ref_genome data/seq.fa --train_data data/training.sorted.bed \\
        --validation_data data/validation.sorted.bed --model_path models/checkpoint_6/model \\
        --model_config_path models/checkpoint_6/model.config.pkl --train_all \\
        --init_fc_with_pretrained --experiment_name example4 > test4.out 2> test4.err
        """)
    )

    # Register common arguments
    transfer_required, transfer_data_args, transfer_model_args, transfer_learn_args, transfer_raytune_args = add_common_transfer_parser(transfer_parser)

    # Add snv-specific arguments
    transfer_model_args.add_argument('--n_class', type=int, metavar='INT', default=4,  
                                     help=textwrap.dedent("""
                                     Number of mutation classes (or types), including the 
                                     non-mutated class. Default: 4.""").strip())
    return transfer_parser