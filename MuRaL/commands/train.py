from typing import Callable, Any
import argparse
import textwrap

def add_common_train_parser(
    train_parser: argparse.ArgumentParser
) -> tuple:
    """
    Add common arguments for all training parsers.
    """

    train_optional = train_parser._action_groups.pop()
    train_optional.title = 'Other arguments'
    train_required = train_parser.add_argument_group('Required arguments')
    data_args = train_parser.add_argument_group('Data-related arguments')
    model_args = train_parser.add_argument_group('Model-related arguments')
    calibra_args = train_parser.add_argument_group('Calibration-related arguments')
    learn_args = train_parser.add_argument_group('Learning-related arguments')
    raytune_args = train_parser.add_argument_group('RayTune-related arguments')

    train_required.add_argument('--ref_genome', type=str, metavar='FILE', default='',  
                          required=True, help=textwrap.dedent("""
                          File path of the reference genome in FASTA format.""").strip())
    
    train_required.add_argument('--train_data', type=str, metavar='FILE', default='',  
                          required=True, help= textwrap.dedent("""
                          File path of training data in a sorted BED format. If the options
                          --validation_data and --valid_ratio not specified, 10%% of the
                          sites sampled from the training BED will be used as the
                          validation data.""").strip())
    
    data_args.add_argument('--validation_data', type=str, metavar='FILE', default=None,
                          help=textwrap.dedent("""
                          File path for validation data. If this option is set,
                          the value of --valid_ratio will be ignored. Default: None.
                          """).strip()) 
    
    data_args.add_argument('--sample_weights', type=str, metavar='FILE', default=None,
                          help=textwrap.dedent("""
                          File path for sample weights. Default: None.
                          """).strip())
    
    data_args.add_argument('--valid_ratio', type=float, metavar='FLOAT', default=0.1, 
                          help=textwrap.dedent("""
                          Ratio of validation data relative to the whole training data.
                          Default: 0.1. 
                          """ ).strip())
    
    data_args.add_argument('--split_seed', type=int, metavar='INT', default=-1, 
                          help=textwrap.dedent("""
                          Seed for randomly splitting data into training and validation
                          sets. Default: a random number generated by the job.
                          """ ).strip())
    
    data_args.add_argument('--bw_paths', type=str, metavar='FILE', default=None,
                          help=textwrap.dedent("""
                          File path for a list of BigWig files for non-sequence 
                          features such as the coverage track. Default: None.""").strip())

    data_args.add_argument('--without_bw_distal', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          Do not use BigWig tracks for distal-related layers. Default: False.""").strip())
    
    data_args.add_argument('--seq_only', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          If set, use only genomic sequences for the model and ignore
                          bigWig tracks. Default: False.""").strip())
    
    data_args.add_argument('--with_h5', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          Output distal encoding in HDF5 File. This parameter can help improve the 
                          speed of data loading in specific situations. Before using this parameter, 
                          please first consider increasing --cpu_per_trial, this is a more effect way 
                          to accelerate training process by speeding up data loading. 
                           Default: False.""").strip())
    
    data_args.add_argument('--h5f_path', type=str, default=None,
                          help=textwrap.dedent("""
                          Specify the folder to generate HDF5. Default: Folder containing the BED file.""").strip())
    
    data_args.add_argument('--n_h5_files', type=int, metavar='INT', default=1, 
                          help=textwrap.dedent("""
                          Number of HDF5 files for each BED file. When the BED file has many
                          positions and the distal radius is large, increasing the value for 
                          --n_h5_files files can reduce the time for generating HDF5 files.
                          Default: 1.                          """ ).strip())
    
    data_args.add_argument('--save_valid_preds', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          Save prediction results for validation data in the checkpoint
                          folders. Default: False.
                          """ ).strip())

    model_args.add_argument('--local_radius', type=int, metavar='INT', default=[5], nargs='+',
                          help=textwrap.dedent("""
                          Radius of the local sequence to be considered in the 
                          model. Length of the local sequence = local_radius*2+1 bp.
                          Default: 5.""" ).strip())
    
    model_args.add_argument('--local_order', type=int, metavar='INT', default=[3], nargs='+', 
                          help=textwrap.dedent("""
                          Length of k-mer in the embedding layer. Default: 3.""").strip())
    
    model_args.add_argument('--distal_radius', type=int, metavar='INT', default=[200], nargs='+', 
                          help=textwrap.dedent("""
                          Radius of the expanded sequence to be considered. 
                          Length of the expanded sequence = distal_radius*2+1 bp.
                          Values should be >=100. Default: 200. 
                          """ ).strip())
    
    model_args.add_argument('--distal_order', type=int, metavar='INT', default=1, 
                          help=textwrap.dedent("""
                          Order of distal sequences to be considered. Kept for 
                          future development. Default: 1. """ ).strip())   

    model_args.add_argument('--CNN_kernel_size', type=int, metavar='INT', default=[3], nargs='+', 
                          help=textwrap.dedent("""
                          Kernel size for CNN layers in the expanded module. Default: 3.
                          """ ).strip())
    
    model_args.add_argument('--CNN_out_channels', type=int, metavar='INT', default=[32], nargs='+', 
                          help=textwrap.dedent("""
                          Number of output channels for CNN layers. Default: 32.
                          """ ).strip())



    calibra_args.add_argument('--poisson_calib', default=False, action='store_true', 
                          help=textwrap.dedent("""
                            Use Poisson calibration for the model. 
                           Default: False.""").strip())

    learn_args.add_argument('--segment_center', type=int, metavar='INT', default=300000, 
                          help=textwrap.dedent("""
                          The maximum encoding unit of the sequence. It affects trade-off 
                          between RAM memory and preprocessing speed. It is recommended to use 300k.
                          Default: 300000.""" ).strip())
    
    learn_args.add_argument('--sampled_segments', type=int, metavar='INT', default=[10], nargs='+',  
                          help=textwrap.dedent("""
                          Number of segments chosen for generating samples for batches in DataLoader.
						  Default: 10.
                          """ ).strip())
    
    learn_args.add_argument('--batch_size', type=int, metavar='INT', default=[128], nargs='+', 
                          help=textwrap.dedent("""
                          Size of mini batches for model training. Default: 128.
                          """ ).strip())
    
    learn_args.add_argument('--custom_dataloader', default=False, action='store_true',  
                          help=textwrap.dedent("""
                          Use a custom data loader. This data loader is not supported parallelizing
                          data loading. For '--cpu-per-trial 1' and without HD5, the speed of loading
                          data is faster than default dataloader. Default: False.
                          """ ).strip())

#    learn_args.add_argument('--ImbSampler', default=False, action='store_true', 
#                          help=textwrap.dedent("""
#                          Use ImbalancedDatasetSampler for dataloader.
#                          """ ).strip())
                          
    learn_args.add_argument('--optim', type=str, metavar='STR', default=['Adam'], nargs='+', 
                          help=textwrap.dedent("""
                          Optimization method for parameter learning: 'Adam' or 'AdamW'.
                          Default: 'Adam'.
                          """ ).strip())
 
    learn_args.add_argument('--learning_rate', type=float, metavar='FLOAT', default=[0.001], nargs='+', 
                          help=textwrap.dedent("""
                          Learning rate for parameter learning, an argument for the 
                          optimization method.  Default: 0.001.
                          """ ).strip())
    
    learn_args.add_argument('--lr_scheduler', type=str, metavar='STR', default=['StepLR'], nargs='+', 
                          help=textwrap.dedent("""
                          Learning rate scheduler.
                          Default: 'StepLR'.
                          """ ).strip())
    
    learn_args.add_argument('--weight_decay_auto', type=float, metavar='FLOAT', default=0.1, 
                          help=textwrap.dedent("""
                          Calcaute 'weight_decay' (regularization parameter) based on total 
                          training steps. It automatically adjusts 'weight_decay' for different 
                          batch sizes, training sizes and epochs. Its value MUST be smaller than 1.
                          For values in the range 0~1, smaller values mean stronger regularization.
                          Set a value of <=0 to turn this off.
                          Default: 0.1.
                          """ ).strip())
    
    learn_args.add_argument('--weight_decay', type=float, metavar='FLOAT', default=[1e-5], nargs='+', 
                          help=textwrap.dedent("""
                          'weight_decay' argument (regularization) for the optimization method. 
                          If you want to use this option, please also set '--weight_decay_auto' to 0.
                          Default: 1e-5. 
                          """ ).strip())
    
    learn_args.add_argument('--restart_lr', type=float, metavar='FLOAT', default=1e-4, 
                          help=textwrap.dedent("""
                          When the learning rate reaches the mininum rate, reset it to 
                          a larger one. Default: 1e-4.
                          """ ).strip())
    
    learn_args.add_argument('--min_lr', type=float, metavar='FLOAT', default=1e-6, 
                          help=textwrap.dedent("""
                          The minimum learning rate. Default: 1e-6.
                          """ ).strip())
    
    learn_args.add_argument('--LR_gamma', type=float, metavar='FLOAT', default=[0.9], nargs='+', 
                          help=textwrap.dedent("""
                          'gamma' argument for the learning rate scheduler.
                           Default: 0.9.
                           """ ).strip())

    learn_args.add_argument('--cudnn_benchmark_false', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          If set, torch.backends.cudnn.benchmark will be False. 
                          Default: not set.
                          """).strip())
    
    raytune_args.add_argument('--use_ray', default=False, action='store_true',
                          help=textwrap.dedent("""
                          Use ray to run multiple trials in parallel.  Default: False.
                          """ ).strip()) 
    
    raytune_args.add_argument('--n_trials', type=int, metavar='INT', default=2, 
                          help=textwrap.dedent("""
                          Number of trials for this training job.  Default: 2.
                          """ ).strip())
    
    raytune_args.add_argument('--epochs', type=int, metavar='INT', default=10, 
                          help=textwrap.dedent("""
                          Maximum number of epochs for each trial.  Default: 10.
                          """ ).strip())
    
    raytune_args.add_argument('--grace_period', type=int, metavar='INT', default=5, 
                          help=textwrap.dedent("""
                          'grace_period' parameter for early stopping. 
                           Default: 5.
                           """ ).strip())
    
    raytune_args.add_argument('--ASHA_metric', type=str, metavar='STR', default='loss', 
                          help=textwrap.dedent("""
                          Metric for ASHA schedualing: 'loss', 'fdiri_loss'.
                          Default: 'loss'.
                          """ ).strip())
    
    raytune_args.add_argument('--ray_ncpus', type=int, metavar='INT', default=2, 
                          help=textwrap.dedent("""
                          Number of CPUs requested by Ray-Tune. Default: 2.
                          """ ).strip())
    
    raytune_args.add_argument('--ray_ngpus', type=int, metavar='INT', default=1, 
                          help=textwrap.dedent("""
                          Number of GPUs requested by Ray-Tune. Default: 1.
                          """ ).strip())
    
    raytune_args.add_argument('--cpu_per_trial', type=int, metavar='INT', default=2, 
                          help=textwrap.dedent("""
                          Number of CPUs used per trial. Default: 2.
                          """ ).strip())
    
    raytune_args.add_argument('--gpu_per_trial', type=float, metavar='FLOAT', default=0.15, 
                          help=textwrap.dedent("""
                          Number of GPUs used per trial. Default: 0.15.
                          """ ).strip())
    
    raytune_args.add_argument('--cuda_id', type=str, metavar='STR', default=None, 
                          help=textwrap.dedent("""
                          Which GPU device to be used. Default: '0'. 
                          """ ).strip())
    
    raytune_args.add_argument('--rerun_failed', default=False, action='store_true', 
                          help=textwrap.dedent("""
                          Rerun errored or incomplete trials. Default: False.
                          """ ).strip())

    train_parser._action_groups.append(train_optional)

    return train_required, model_args, calibra_args, raytune_args

def add_indel_train_parser(subparsers: argparse._SubParsersAction) -> argparse._SubParsersAction:
    """
    Add a parser for training indel models.
    """
    train_parser = subparsers.add_parser(
        'train', 
        help='Train mural-indel model',
        formatter_class=argparse.RawTextHelpFormatter,
        description="""
    Overview
    --------    
    This tool trains MuRaL models with training and validation mutation data
    and exports training results under the "./ray_results/" folder.
    
    * Input data
    Input data files include the reference sequence file (FASTA format, required), 
    a training data file (required) and a validation data file (optional). If the 
    validation data file isn't provided, a fraction of the sites from the training 
    data file are used as validation data.
    
    Input training and validation data files must be in BED format
    (more info about BED at https://genome.ucsc.edu/FAQ/FAQformat.html#format1). 
    
    Some example lines of an input BED file are shown below.
    chr1	2333436	2333437	.	0	+
    chr1	2333446	2333447	.	2	-
    chr1	2333468	2333469	.	1	-
    chr1	2333510	2333511	.	6	-
    chr1	2333812	2333813	.	0	-
    
    In the BED-formatted lines above, the 5th column is used to represent mutation
    status: usually, '0' means the non-mutated status and other numbers means 
    specific mutation types (e.g. '1' for '1bp insertion/deletion(INDEL)', '2' for 
    2bp INDEL, ..., '6' for 6 bp INDEL, '7' for 7-20 bp INDEL). You can
    specify an arbitrary order for a group of mutation types with incremental 
    numbers starting from 0, but make sure that the same order is consistently 
    used in training, validation and testing datasets. 
    
    Importantly, the training and validation BED file MUST be SORTED by chromosome
    coordinates. You can sort BED files by 'bedtools sort' or 'sort -k1,1 -k2,2n'.
    
    * Output data
    This tool saves the model information at each checkpoint, normally at the 
    end of each training epoch of each trial. 
        
		* If executing multiple trials serially (default) or 
          running only a single trial(use '--n_trial 1'):
        The checkpointed model files during training are saved under folders 
        named like:

        ./results/your_experiment_name/Train_xxx...xxx/checkpoint_x/
              - model
              - model.config.pkl
              - model.fdiri_cal.pkl

       
        * If execute multiple trials in parallel(use '--use_ray'):
        The checkpointed model files during training are saved under folders 
        named like:
        
        ./ray_results/your_experiment_name/Train_xxx...xxx/checkpoint_x/
            - model
            - model.config.pkl
            - model.fdiri_cal.pkl

    In the above folder, the 'model' file contains the learned model parameters. 
    The 'model.config.pkl' file contains configured hyperparameters of the model.
    The 'model.fdiri_cal.pkl' file (if exists) contains the calibration model 
    learned with validation data, which can be used for calibrating predicted 
    mutation rates. These files can be used in downstream analyses such as
    model prediction and transfer learning.
         
    Note: If your machine has sufficient resources to execute multiple trials in parallel, 
	it is recommended to add the --use_ray option. Using Ray allows for better resource 
	scheduling. If executing multiple trials serially or running only a single trial (--n_trials 1), 
	it is recommended not to use --use_ray, which can improve the runtime speed by approximately 
	2-3 times for each trial.
   
    Command line examples
    --------------------- 
    1. The following command will train a model by running two trials, using data in
    'train.sorted.bed' for training. The training results will be saved under the
    folder './ray_results/example1/'. Default values will be used for other
    unspecified arguments. Note that, by default, 10% of the sites sampled from 
    'train.sorted.bed' is used as validation data (i.e., '--valid_ratio 0.1').
        
		# running two trials without Ray
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --n_trials 2 --experiment_name example1 > test1.out 2> test1.err
        
        # running two trials using Ray 
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --n_trials 2 --use_ray --experiment_name example1 > test1.out 2> test1.err

    2. The following command will use data in 'train.sorted.bed' as training
    data and a separate 'validation.sorted.bed' as validation data. The option
    '--local_radius 10' means that length of the local sequence used for training
    is 10*2+1 = 21 bp. '--distal_radius 100' means that length of the expanded 
    sequence used for training is 100*2+1 = 201 bp. 
    
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --validation_data validation.sorted.bed --n_trials 2 --local_radius 10 \\ 
        --distal_radius 100 --experiment_name example2 > test2.out 2> test2.err
    
    3. If you don't have (or don't want to use) GPU resources, you can set options
    '--ray_ngpus 0 --gpu_per_trial 0' as below. Be aware that if training dataset 
    is large or the model is parameter-rich, CPU-only computing could take a very 
    long time!
    
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --n_trials 2 --ray_ngpus 0 --gpu_per_trial 0 --experiment_name example3 \\ 
        > test3.out 2> test3.err
    
    4. If the length of the expanded sequence used for training is large (greater than 1000),
    data loading becomes a bottleneck in the training process. You can set the option 
    '--cpu_per_trial' to specify how many CPUs each trial should be used to do data loading.

        mural_train --ref_genome seq.fa --train_data train.sorted.bed --n_trials 2 \\
		--cpu_per_trial 4 --experiment_name example4 > test4.out 2> test4.err
      
    Notes
    -----
    1. The training and validation BED file MUST BE SORTED by chromosome 
    coordinates. You can sort BED files by running 'bedtools sort' or 
    'sort -k1,1 -k2,2n'.
    
    2. For '--with_h5', this tool generates an HDF5 file for each input BED
    file (training or validation file) based on the value of '--distal_radius' 
    and the tracks in '--bw_paths', if the corresponding HDF5 file doesn't 
    exist or is corrupted. Make sure that you have write permission for the folder
    containing the BED file(s). Only one job is allowed to write to an HDF5 file,
    so don't run multiple jobs involving a same BED file when its HDF5 file 
    isn't generated yet. Otherwise, it may cause file permission errors.
    
    3. If it takes long to finish a job, you can check the information exported 
    to stdout (or redirected file) for the progress during running. 
    """)
    # Register common arguments
    train_required, model_args, calibra_args, raytune_args = add_common_train_parser(train_parser)


    # Add indel-specific arguments
    model_args.add_argument('--model_no', type=int, metavar='INT', default=2, 
                            help=textwrap.dedent("""
                            Which network architecture to be used: 
                            0 - 'Unet-Small' model;
                            Default: 0.
                            """ ).strip())
    
    model_args.add_argument('--n_class', type=int, metavar='INT', default=8,  
                            help=textwrap.dedent("""
                            Number of mutation classes (or types), including the 
                            non-mutated class. Default: 8.""").strip())

    model_args.add_argument('--down_list', type=int, metavar='INT', default=1, nargs='+', required=False,
                          help=textwrap.dedent("""
                          dowsample list for UNet_pro. Default: 1.
                          """ ).strip())

    raytune_args.add_argument('--experiment_name', type=str, metavar='STR', default='indel_experiment',
                              help=textwrap.dedent("""
                              Ray-Tune experiment name.  Default: 'indel_experiment'.
                              """ ).strip()) 

    return train_parser


def add_snv_train_parser(subparsers: argparse._SubParsersAction) -> argparse._SubParsersAction:
    """
    Add a parser for training snv models.
    """
    train_parser = subparsers.add_parser('train', 
    help='Train mural-snv model',
    formatter_class=argparse.RawTextHelpFormatter,
    description="""
    Overview
    --------    
    This tool trains MuRaL models with training and validation mutation data
    and exports training results under the "./ray_results/" folder.
    
    * Input data
    Input data files include the reference sequence file (FASTA format, required), 
    a training data file (required) and a validation data file (optional). If the 
    validation data file isn't provided, a fraction of the sites from the training 
    data file are used as validation data.
    
    Input training and validation data files must be in BED format
    (more info about BED at https://genome.ucsc.edu/FAQ/FAQformat.html#format1). 
    
    Some example lines of an input BED file are shown below.
    chr1	2333436	2333437	.	0	+
    chr1	2333446	2333447	.	2	-
    chr1	2333468	2333469	.	1	-
    chr1	2333510	2333511	.	3	-
    chr1	2333812	2333813	.	0	-
    
    In the BED-formatted lines above, the 5th column is used to represent mutation
    status: usually, '0' means the non-mutated status and other numbers means 
    specific mutation types (e.g. '1' for 'A>C', '2' for 'A>G', '3' for 'A>T'). You can
    specify an arbitrary order for a group of mutation types with incremental 
    numbers starting from 0, but make sure that the same order is consistently 
    used in training, validation and testing datasets. 
    
    Importantly, the training and validation BED file MUST be SORTED by chromosome
    coordinates. You can sort BED files by 'bedtools sort' or 'sort -k1,1 -k2,2n'.
    
    * Output data
    This tool saves the model information at each checkpoint, normally at the 
    end of each training epoch of each trial. 
        
		* If executing multiple trials serially (default) or 
          running only a single trial(use '--n_trial 1'):
        The checkpointed model files during training are saved under folders 
        named like:

        ./results/your_experiment_name/Train_xxx...xxx/checkpoint_x/
              - model
              - model.config.pkl
              - model.fdiri_cal.pkl

       
        * If execute multiple trials in parallel(use '--use_ray'):
        The checkpointed model files during training are saved under folders 
        named like:
        
        ./ray_results/your_experiment_name/Train_xxx...xxx/checkpoint_x/
            - model
            - model.config.pkl
            - model.fdiri_cal.pkl

    In the above folder, the 'model' file contains the learned model parameters. 
    The 'model.config.pkl' file contains configured hyperparameters of the model.
    The 'model.fdiri_cal.pkl' file (if exists) contains the calibration model 
    learned with validation data, which can be used for calibrating predicted 
    mutation rates. These files can be used in downstream analyses such as
    model prediction and transfer learning.
         
    Note: If your machine has sufficient resources to execute multiple trials in parallel, 
	it is recommended to add the --use_ray option. Using Ray allows for better resource 
	scheduling. If executing multiple trials serially or running only a single trial (--n_trials 1), 
	it is recommended not to use --use_ray, which can improve the runtime speed by approximately 
	2-3 times for each trial.
   
    Command line examples
    --------------------- 
    1. The following command will train a model by running two trials, using data in
    'train.sorted.bed' for training. The training results will be saved under the
    folder './ray_results/example1/'. Default values will be used for other
    unspecified arguments. Note that, by default, 10% of the sites sampled from 
    'train.sorted.bed' is used as validation data (i.e., '--valid_ratio 0.1').
        
		# running two trials without Ray
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --n_trials 2 --experiment_name example1 > test1.out 2> test1.err
        
        # running two trials using Ray 
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --n_trials 2 --use_ray --experiment_name example1 > test1.out 2> test1.err

    2. The following command will use data in 'train.sorted.bed' as training
    data and a separate 'validation.sorted.bed' as validation data. The option
    '--local_radius 10' means that length of the local sequence used for training
    is 10*2+1 = 21 bp. '--distal_radius 100' means that length of the expanded 
    sequence used for training is 100*2+1 = 201 bp. 
    
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --validation_data validation.sorted.bed --n_trials 2 --local_radius 10 \\ 
        --distal_radius 100 --experiment_name example2 > test2.out 2> test2.err
    
    3. If you don't have (or don't want to use) GPU resources, you can set options
    '--ray_ngpus 0 --gpu_per_trial 0' as below. Be aware that if training dataset 
    is large or the model is parameter-rich, CPU-only computing could take a very 
    long time!
    
        mural_train --ref_genome seq.fa --train_data train.sorted.bed \\
        --n_trials 2 --ray_ngpus 0 --gpu_per_trial 0 --experiment_name example3 \\ 
        > test3.out 2> test3.err
    
    4. If the length of the expanded sequence used for training is large (greater than 1000),
    data loading becomes a bottleneck in the training process. You can set the option 
    '--cpu_per_trial' to specify how many CPUs each trial should be used to do data loading.

        mural_train --ref_genome seq.fa --train_data train.sorted.bed --n_trials 2 \\
		--cpu_per_trial 4 --experiment_name example4 > test4.out 2> test4.err
      
    Notes
    -----
    1. The training and validation BED file MUST BE SORTED by chromosome 
    coordinates. You can sort BED files by running 'bedtools sort' or 
    'sort -k1,1 -k2,2n'.
    
    2. For '--with_h5', this tool generates an HDF5 file for each input BED
    file (training or validation file) based on the value of '--distal_radius' 
    and the tracks in '--bw_paths', if the corresponding HDF5 file doesn't 
    exist or is corrupted. Make sure that you have write permission for the folder
    containing the BED file(s). Only one job is allowed to write to an HDF5 file,
    so don't run multiple jobs involving a same BED file when its HDF5 file 
    isn't generated yet. Otherwise, it may cause file permission errors.
    
    3. If it takes long to finish a job, you can check the information exported 
    to stdout (or redirected file) for the progress during running. 
    """)

    # Register common arguments
    train_required, model_args, calibra_args, raytune_args = add_common_train_parser(train_parser)

    # Add snv-specific arguments
    model_args.add_argument('--model_no', type=int, metavar='INT', default=2, 
                            help=textwrap.dedent("""
                            Which network architecture to be used: 
                            0 - 'local-only' model;
                            1 - 'expanded-only' model;
                            2 - 'local + expanded' model.
                            3 - 'local + expanded' model, with separate local FC layers for
                                features in BigWig tracks.
                            Default: 2.
                            """ ).strip())
    
    model_args.add_argument('--n_class', type=int, metavar='INT', default=4,  
                            help=textwrap.dedent("""
                            Number of mutation classes (or types), including the 
                            non-mutated class. Default: 4.""").strip())

    model_args.add_argument('--local_hidden1_size', type=int, metavar='INT', default=[150], nargs='+', 
                          help=textwrap.dedent("""
                          Size of 1st hidden layer for local module. Default: 150.
                          """).strip())
    
    model_args.add_argument('--local_hidden2_size', type=int, metavar='INT', default=[0], nargs='+',
                          help=textwrap.dedent("""
                          Size of 2nd hidden layer for local module. 
                          Default: local_hidden1_size//2 .
                          """ ).strip())

    model_args.add_argument('--emb_dropout', type=float, metavar='FLOAT', default=[0.1], nargs='+', 
                          help=textwrap.dedent("""
                          Dropout rate for inputs of the k-mer embedding layer.
                          Default: 0.1.""" ).strip())
    
    model_args.add_argument('--local_dropout', type=float, metavar='FLOAT', default=[0.1], nargs='+', 
                          help=textwrap.dedent("""
                          Dropout rate for inputs of local hidden layers.  Default: 0.1.""" ).strip())

    model_args.add_argument('--distal_fc_dropout', type=float, metavar='FLOAT', default=[0.25], nargs='+', 
                          help=textwrap.dedent("""
                          Dropout rate for the FC layer of the expanded module.
                          Default: 0.25.
                           """ ).strip())

    raytune_args.add_argument('--experiment_name', type=str, metavar='STR', default='snv_experiment',
                              help=textwrap.dedent("""
                              Ray-Tune experiment name.  Default: 'snv_experiment'.
                              """ ).strip()) 

    return train_parser
