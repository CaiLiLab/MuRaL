#!/usr/bin/env python

import sys
import re
import gzip

import pandas as pd
import numpy as np
from pandas.core.frame import DataFrame
import argparse
from scipy.stats import pearsonr
from typing import Dict, Tuple, Any



def parse_arguments(parser):
    parser.add_argument('--pred_file', type=str, help='base-wise mutation rate prediction file generated by MuRaL.')
    parser.add_argument('--window_size', type=int, default=100000, help='window size (bp) for calculating regional rates. Default: 100000.')
    parser.add_argument('--ratio_cutoff', type=float, default=0.2, help='ratio cutoff for filtering windows with few valid sites. Default: 0.2, meaning that windows with  fewer than 0.2*median(numbers of sites in surveyed windows) will be discarded.')
    parser.add_argument('--n_class', type=int, default=8, help="Number of mutation classes (or types), including the non-mutation type. Default: 8")
    parser.add_argument('--out_prefix', type=str, help='name prefix for output files storing regional mutation rates and correlations')
    args = parser.parse_args()
    return args

class RegionMutSaver:
    def __init__(self, n_class):
        self.n_class = n_class
        self.region_mut_obs = {}
        self.region_mut_pred = {}
    
    def add_obs(self, region, mut_type):
        if region not in self.region_mut_obs:
            self.region_mut_obs[region] = np.zeros(self.n_class)
        self.region_mut_obs[region][mut_type] += 1
    
    def add_pred(self, region, probs):
        if region not in self.region_mut_pred:
            self.region_mut_pred[region] = np.zeros(self.n_class)
        for i in range(self.n_class):
            self.region_mut_pred[region][i] += probs[i]


def filt_region(df: DataFrame, ratio_cutoff: float) -> DataFrame:
    """
    Filter out regions with insufficient number of sites.
    Args:
        df: DataFrame containing mutation rates and counts
        ratio_cutoff: ratio cutoff for filtering windows with few valid sites
    Returns:
        Filtered DataFrame
    """
    # Calculate the cutoff for site counts
    site_count_cutoff = win_cut_cal(df['number_of_all'].values, ratio_cutoff)

    # Filter the DataFrame based on the cutoff
    df['used_or_deprecated'] = np.where(
        df['number_of_all'] >= site_count_cutoff,
        'used',
        'deprecated'
        )
    return df

def win_cut_cal(site_counts, ratio_cutoff):
    """
    calculate the cutoff to discard window with insufficient number of sites
    args:
        site_counts: list or array, each element is the number of sites in a window
    """
    site_count_cutoff = ratio_cutoff*np.median(site_counts)
    return site_count_cutoff


def record_region_mut(chrom: str, window_end: int, mut_type: int, probs: list, data_saver) -> None:
    """Process mutation site statistics for a single k-mer"""
    # get region keys
    region_key = (chrom, window_end)

    # Update region counts
    data_saver.add_obs(region_key, mut_type)
    data_saver.add_pred(region_key, probs)


def calculate_mutation_rates(region_mut_saver: Any) -> pd.DataFrame:
    """Calculate observed and predicted mutation rates for genomic regions.
    
    Args:
        region_mut_saver: Data container with:
            - region_mut_obs: Dict[(chrom, window_end), np.ndarray] of observed mutation counts
            - region_mut_pred: Dict[(chrom, window_end), np.ndarray] of predicted mutation counts  
            - n_class: Number of mutation classes
    
    Returns:
        DataFrame with columns:
        - chrom: Chromosome name
        - window_end: Window end position
        - avg_obs_rate{1..n}: Average observed rates per class (float)
        - avg_pred_rate{1..n}: Average predicted rates per class (float)
        - number_of_mut{1..n}: Raw mutation counts per class (int)
        - number_of_all: Total counts (int)
    """
    # Generate column names
    mutation_classes = range(1, region_mut_saver.n_class)
    rate_cols = [f"avg_obs_rate{i}" for i in mutation_classes]
    pred_cols = [f"avg_pred_rate{i}" for i in mutation_classes]
    count_cols = [f"number_of_mut{i}" for i in mutation_classes]
    
    # Prepare data containers
    chroms = []
    window_ends = []
    data = []
    
    # Process each genomic region
    for (chrom, window_end), obs_counts in region_mut_saver.region_mut_obs.items():
        total = obs_counts.sum()
        chroms.append(chrom)
        window_ends.append(window_end)
        
        data.append(np.concatenate([
            obs_counts[1:] / total,  # Normalized observed rates
            region_mut_saver.region_mut_pred[(chrom, window_end)][1:] / total,  # Normalized predicted rates
            obs_counts[1:],  # Raw mutation counts
            [total]  # Total counts
        ]))
    
    # Create DataFrame
    df = pd.DataFrame(
        data=data,
        columns=rate_cols + pred_cols + count_cols + ['number_of_all']
    )
    
    # Add chromosome and position columns
    df.insert(0, 'chrom', chroms)
    df.insert(1, 'window_end', window_ends)
    
    # Convert count columns to appropriate types
    df[count_cols] = df[count_cols].astype('uint32')
    df['number_of_all'] = df['number_of_all'].astype('uint64')
    df[rate_cols + pred_cols] = df[rate_cols + pred_cols].astype('float32')
    
    return df

def calculate_correlations(df: pd.DataFrame, n_class) -> Dict[int, Tuple[float, float]]:
    """Calculate Pearson correlations for each mutation subtype"""

    return {
        subtype: pearsonr(df[f'avg_obs_rate{subtype}'], df[f'avg_pred_rate{subtype}'])
        for subtype in range(1, n_class)
    }

# ----------------------
# Output Handling
# ----------------------
def write_output_files(args, df: pd.DataFrame, 
                      correlations: Dict[int, Tuple[float, float]]) -> None:
    """Write results to output files"""
    # Write mutation rates
    window_size = int(args.window_size) / 1000
    window = f"{int(window_size)}Kb"
    df.to_csv(
        f"{args.out_prefix}.{window}.mut_rates.tsv",
        sep='\t', index=False)
    
    # Write correlation results
    with open(f"{args.out_prefix}.{window}.corr.txt", 'w') as f:
        for subtype, (corr, pval) in correlations.items():
            f.write(f"{window}\t{subtype}\t{corr:.5f}\t{pval:.10e}\n")

def run_regional_corr_calc(args):
    window_size = int(args.window_size)
    ratio_cutoff = float(args.ratio_cutoff)
    n_class = args.n_class
    prob_mut_type_list = [f'prob{i}' for i in range(1, n_class)] 
    region_mut_saver = RegionMutSaver(n_class)

    opener = gzip.open if args.pred_file.endswith('.gz') else open
    # save regional count of obs and predict
    with opener(args.pred_file, 'rt') as obs_file:
        # header process
        header = next(obs_file)
        if not header.startswith('chrom'):
            raise ValueError(f"Invalid file header: {header.strip()}, header should be continue with 'chrom'")
            # Parse line
        # check file is consistent with parameter
        header = header.strip().split('\t')
        if len(header) != n_class + 5:
            raise ValueError(
                f"Column count mismatch. Expected {n_class + 5} columns, "
                f"got {len(len(header))} in line: {header}")

        for line in obs_file:
            line = line.strip().split('\t')
            chrom, start, end, strand, mut = line[:5] 
            probs = np.asarray(line[5:], dtype='float') # prob0 to prob n
            start, end, mut = int(start), int(end), int(mut)

            window_end = start // window_size * window_size + window_size # located window of each site
            # Process site statistics
            record_region_mut(
                chrom=chrom,
                window_end=window_end,
                mut_type=mut,
                probs=probs,
                data_saver=region_mut_saver
            )
    # Calculate and save results
    results_df = calculate_mutation_rates(region_mut_saver)
    # cut off
    results_df = filt_region(results_df, ratio_cutoff)
    # calc correlation
    filt_df = results_df[results_df['used_or_deprecated'] == 'used']
    corr_results = calculate_correlations(filt_df, n_class)
    # save results
    write_output_files(args, results_df, corr_results)
        


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='regional correlation calculator')
    args = parse_arguments()
    run_regional_corr_calc(args)
